<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HNN5E2KR1W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HNN5E2KR1W');
</script>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="../minimal.css">
	<!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
	<title>Liverpool Hope Department of Psychology Labs and Resources</title>
</head>

<body>
	<div class="wrapper">
		<header>
			<h2><a href="../index.html">Psychology Labs and Resources</a></h2>
			<img src="../images/Hope.png" alt="Hope University Logo">
			<h3><a href="https://www.hope.ac.uk/psychology/">Department of Psychology</a></h3>
			<ul>
				<li><a href="../labs.html">Lab Information</a></li>
				<li><a href="../form.html">Lab Access Form</a></li>
				<li> <a href="../book.html">Book a Lab</a></li>
				<li><a href="../equipment.html">Psychology Equipment</a></li>
				<li><a href="../psychometrics.html">Psychometric Tests &amp; Tools</a></li>
				<li> <a href="../experimentbank.html">PsychoPy Experiment Bank</a></li>
				<li> <a href="../stimulusbanks.html">Stimulus Banks</a></li>
				<li> <a href="../qualtrics.html">Qualtrics</a></li>
				<li> <a href="../psychopy.html">PsychoPy Builder</a></li>
				<li> <a href="../stats.html">Lab Manuals &amp; Software</a></li>
				<li> <a href="../sona.html">Participant Scheme (SONA)</a></li>
			</ul>
		</header>
		<section>
			<h1 id="#top">Human Face Databases</h1>
			<h4>
				<a href="#1-9">0-9 |</a>
				<a href="#a">A |</a>
				<a href="#b">B |</a>
				<a href="#c">C |</a>
				<a href="#d">D |</a>
				<a href="#e">E |</a>
				<a href="#f">F |</a>
				<a href="#g">G |</a>
				<a href="#h">H |</a>
				<a href="#i">I |</a>
				<a href="#j">J |</a>
				<a href="#k">K |</a>
				<a href="#l">L |</a><br>
				<a href="#m">M |</a>
				<a href="#n">N |</a>
				<a href="#o">O |</a>
				<a href="#p">P |</a>
				<a href="#q">Q |</a>
				<a href="#r">R |</a>
				<a href="#s">S |</a>
				<a href="#t">T |</a>
				<a href="#u">U |</a>
				<a href="#v">V |</a>
				<a href="#w">W |</a>
				<a href="#x">X |</a>
				<a href="#y">Y |</a>
				<a href="#z">Z |</a>
			</h4>






			<h2>Attribution</h2>
			<p>This page has been adapted from the <a href="https://libguides.princeton.edu/facedatabases">Princeton University Library Face Image Databases</a> by <a href="https://libguides.princeton.edu/prf.php?id=5c5c5cb2-7cdb-11ed-9922-0ad758b798c3">Meghan Testerman</a> <a rel="license" href="https://creativecommons.org/licenses/by/4.0/"></a> which is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. <img alt="Creative Commons Licence" style="border-width:0" src="https://licensebuttons.net/l/by/4.0/88x31.png"></p>
			<h2>Use of Face Stimulus Databases</h2>
			<p>The following is a directory of alphabetically listed databases containing face stimulus sets available for use in behavioural research studies. Please read the rights, permissions, licensing information on the database's webpage before proceeding with use. Make sure to obtain the permissions required and credit/cite as requested by the creators.</p>
			&nbsp;<hr>&nbsp;

			<h2 id="1-9">10k US Adult Faces Database</h2>
			<a class="button" target="_blank" href="http://wilmabainbridge.com/facememorability2.html">Link</a>
			<h2>Description</h2>
			<p>This database contains 10,168 natural face photographs and several measures for 2,222 of the faces, including memorability scores, computer vision and psychology attributes, and landmark point annotations. The face photographs are JPEGs with 72 pixels/in resolution and 256-pixel height.</p>
			<h2>Reference</h2>
			<p>Bainbridge, W.A., Isola, P., &amp; Oliva, A. (2013). The intrinsic memorability of face images. <em>Journal of Experimental Psychology: General. Journal of Experimental Psychology: General, 142</em>(4), 1323-1334.</p>
			<h2>Contact</h2>
			<p>Wilma Bainbridge: brainbridgelab@gmail.com</p>
			<h2>Attribution</h2>
			<p>Permission required for access via online form. Always use citation</p>
			<div class="figheading">Fig 1. Exemplar face images from the 10k US Adult Faces Database</div>
			<img src="images/10Kfaces.png" alt="exemplar face images from the 10k US Adult Faces Database">

			&nbsp;<hr>&nbsp;
			<h2 id="a">American Multiracial Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://osf.io/qsdrp/">Link</a>
			<h2>Description</h2>
			<p>This database contains 110 faces (smiling and neutral expression poses) with mixed-race heritage and accompanying ratings of those faces by naive observers that are freely available to academic researchers. The faces were rated on attractiveness, emotional expression, racial ambiguity, masculinity, racial group membership(s), gender group membership(s), warmth, competence, dominance, and trustworthiness.</p>
			<h2>Reference</h2>
			<p>Chen, J.M., Norman, J.B. &amp; Nam, Y. Broadening the stimulus set: Introducing the American Multiracial Faces Database. Behav Res (2020). <a href="https://doi.org/10.3758/s13428-020-01447-8">doi.org/10.3758/s13428-020-01447-8</a></p>
			<h2>Contact</h2>
			<p><a href="https://jacquelinemchen.wixsite.com/sciplab/face-database">https://jacquelinemchen.wixsite.com/sciplab/face-database</a></p>
			<h2>Attribution</h2>
			<p>The AMFD is free to use for academic research. It is subject to a Creative Commons Attribution 4.0 International Public License.</p>
			<div class="figheading">Fig 2. Exemplar face images from the 10k US Adult Faces Database</div>
			<img src="images/AmericanMultiRacial.png" alt="exemplar face images from the 10k US Adult Faces Database">
			&nbsp;<hr>&nbsp;




			<h2>Amsterdam Dynamic Facial Expression Set (ADFES)  <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://aice.uva.nl/research-tools/adfes-stimulus-set/adfes-stimulus-set.html">Link</a>
			<h2>Description</h2>
			<p>ADFES contains filmed emotional expressions from 22 Northern-European and Mediterranean models (10 female/12 male). The set features displays of nine emotions: the six basic emotions (anger, disgust, fear, joy, sadness, and surprise), as well as contempt, pride and embarrassment. </p>
			<h2>Reference</h2>
			<p>Van der Schalk, J., Hawk, S. T., Fischer, A. H., &amp; Doosje, B. J. (in press).Moving faces, looking places: The Amsterdam Dynamic Facial Expressions Set (ADFES), Emotion.</p>
			<h2>Contact</h2>
			<p>Agneta Fisher, a.h.fischer@uva.nl</p>
			<h2>Attribution</h2>
			<p>CC-By Attribution 4.0 International- <a href="https://aice.uva.nl/research-tools/adfes-stimulus-set/request-for-use/request-for-use.html">Request permission</a></p>
			<div class="figheading">Fig 3. Exemplar faces from the ADFES</div>
			<img src="images/adfes.png" alt="exemplar faces from the ADFES">
			&nbsp;<hr>&nbsp;

			<h2>AT&amp;T Databases of Faces <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://git-disl.github.io/GTDLBench/datasets/att_face_dataset/">Link</a>
			<h2>Description</h2>
			<p>This database contains a set of face images taken between April 1992 and April 1994. There are ten different images of each of 40 distinct individuals. For some individuals, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</p>
			<h2>Reference</h2>
			<p>Samaria, F. S. (1994). <i>Face recognition using hidden Markov models</i> (Doctoral dissertation, University of Cambridge).</p>
			<h2>Contact</h2>
			<p>AT&amp;T Laboratories Cambridge</p>
			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 4. Exemplars from the A, T &amp; T Database</div>
			<img src="images/At&T.png" alt="Exemplars from the A, T & T Database">
			&nbsp;<hr>&nbsp;



			<h2 id="b">Basel Face Database (BFD) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://bfd.unibas.ch/en/">Link</a>
			<h2>Description</h2>
			<p>The Basel Face Database is built upon portrait photographs of forty different individuals. All these photographs have been manipulated to appear more or less agentic and communal (Big Two personality dimensions) as well as open to experience, conscientious, extraverted, agreeable, and neurotic (Big Five personality dimensions). Thus, the database consists of forty photographs of different individuals and 14 variations of each of them signaling different personalities. Using this database therefore allows to investigate the impact of personality on different outcome variables in a very systematic way.</p>
			<h2>Reference</h2>
			<p>Walker, M., Sch&ouml;nborn, S., Greifeneder, R., &amp; Vetter, T. (2018). The Basel Face Database: A validated set of photographs reflecting systematic differences in Big Two and Big Five personality dimensions. <em>PloS one, 13</em>(3). doi: https://doi.org/10.1371/journal.pone.0193190</p>
			<h2>Contact</h2>
			<p>Mirella Walker</p>
			<h2>Attribution</h2>
			<p><a href="https://form.psychologie.unibas.ch/public/bfd-materials-request/">Request permission for scientific use.</a></p>
			<div class="figheading">Fig 5. Exemplars from the Basel Face Database</div>
			<img src="images/Basel.png" alt="Exemplars from the Basel Face Database">
			&nbsp;<hr>&nbsp;

			<h2>Bogazici Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192018">Link</a>
			<h2>Description</h2>
			<p>The Bogazici Face Database contains images of Turkish undergraduate student targets. High-resolution standardized photographs were taken and supported by the following materials: (a) basic demographic and appearance-related information, (b) two types of landmark configurations (for Webmorph and geometric morphometrics (GM)), (c) facial width-to-height ratio (fWHR) measurement, (d) information on photography parameters, (e) perceptual norms provided by raters.</p>

			<h2>Reference</h2>
			<p>Saribay SA, Biten AF, Meral EO, Aldan P, Třebick&yacute; V, Kleisner K (2018) The Bogazici face database: Standardized photographs of Turkish faces with supporting materials. <em>PLoS ONE 13</em>(2): e0192018. https://doi.org/10.1371/journal.pone.0192018</p>
			<h2>Contact</h2>
			<p></p>
			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 6.Exemplars from the Bogazici Face Database</div>
			<img src="images/bogazici.png" alt="Exemplars from the Bogazici Face Database">
			&nbsp;<hr>&nbsp;
			
			<h2 id="c">CalTech 10k Web Faces <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://www.vision.caltech.edu/datasets/caltech_10k_webfaces/">Link</a>
			<h2>Description</h2>
			<p>The Caltech database contains images of people collected from the web by typing common given names into Google Image Search. The coordinates of the eyes, the nose and the center of the mouth for each frontal face are provided in a ground truth file. This information can be used to align and crop the human faces or as a ground truth for a face detection algorithm. The dataset has 10,524 human faces of various resolutions and in different settings, e.g. portrait images, groups of people, etc. Profile faces or very low-resolution faces are not labeled.</p>

			<h2>Reference</h2>
			<p>Anelia Angelova, Yaser Abu-Mostafa, Pietro Perona, Pruning Training Sets for Learning of Object Categories , Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005</p>

			<h2>Contact</h2>
			<p>Anelia Angelova, anelia@caltech.edu</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 7. Exemplars from the CalTech 10k Web Faces Database</div>
			<img src="images/caltech.jpeg" alt="Exemplars from the CalTech 10k Web Faces Database">
			&nbsp;<hr>&nbsp;


			<h2>Chicago Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://www.chicagofaces.org/">Link</a>
			<h2>Description</h2>
			<p>The CFD is intended for use in scientific research. It provides high-resolution, standardised photographs of male and female faces of varying ethnicity between the ages of 17-65. Extensive norming data are available for each individual model. These data include both physical attributes (e.g., face size) as well as subjective ratings by independent judges (e.g., attractiveness). The database consists of a main image set and several extension sets.</p>
			<p>The main CFD set consists of images of 597 unique individuals. They include self-identified Asian, Black, Latino, and White female and male models, recruited in the United States. All models are represented with neutral facial expressions. A subset of the models is also available with happy (open mouth), happy (closed mouth), angry, and fearful expressions.</p>
			<h3>CFD-MR</h3>
			<p>The CFD-MR extension set includes images of 88 unique individuals, who self-reported multiracial ancestry. All models were recruited in the United States. The images depict models with neutral facial expressions. Additional facial expression images with happy (open mouth), happy (closed mouth), angry, and fearful expressions are in production and will become available with a future update of the database.</p>
			<h3>CFD-INDIA</h3>
			<p>The CFD-INDIA extension set includes images of 142 unique individuals, recruited in Delhi, India. The images depict models with neutral facial expressions. Additional facial expression images with happy (open mouth), happy (closed mouth), angry, and fearful expressionsare in production and will become available with a future update of the database. </p>

			<h2>References</h2>
			<p>Ma, D. S., Correll, J., &amp; Wittenbrink, B. (2015). The Chicago face database: A free stimulus set of faces and norming data. <em>Behavior research methods, 47</em>(4), 1122-1135.</p>
			<p>Wittenbrink, (2020). Chicago Face Database: Multiracial Expansion. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01482-5.</p>
			<p>Lakshmi, Wittenbrink, Correll, &amp; Ma (2020). The India Face Set: International and Cultural Boundaries Impact Face Impressions and Perceptions of Category Membership. Frontiers in Psychology, 12, 161. https://doi.org/10.3389/fpsyg.2021.627678.</p>
			<h2>Contact</h2>
			<p>Bernd Wittenbrink, bernd.wittenbrink@chicagobooth.edu</p>
			<h2>Attribution</h2>
			<p>The CFD and its expansion sets are a free resource for the scientific community. The database photographs and their accompanying information may be used free of charge for non-commercial scientific research purposes only. The database materials cannot be re-distributed or published without written consent from the copyright holder, the University of Chicago, Center for Decision Research.</p>
			<div class="figheading">Fig 8. Exemplars from the Chicago Database</div>
			<img src="images/chicago.png" alt="Exemplars from the Chicago Database">
			&nbsp;<hr>&nbsp;


			<h2>Child Affective Facial Expression Set (CAFE) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://www.childstudycenter-rutgers.com/the-child-affective-facial-expression-se">Link</a>
			<h2>Description</h2>
			<p>The Child Affective Facial Expressions Set (CAFE) is the first large and representative set of children posing a variety of affective facial expressions that can be used for scientific research. The set is made up of nearly 1200 photographs of over 100 children  (ages 2-8) making 7 different facial expressions - happy, angry, sad, fearful, surprise, neutral, and disgust.</p>
			<h2>Reference</h2>
			<p>LoBue, V. &amp; Thrasher, C. (2015). The Child Affective Facial Expression (CAFE) Set: Validity and reliability from untrained adults. Frontiers in Emotion Science, 5. </p>
			<h2>Contact</h2>
			<p></p>
			<h2>Attribution</h2>
			<p><a href="https://nyu.databrary.org/volume/30">Apply for use https://nyu.databrary.org/volume/30 </a></p>
			<div class="figheading">Fig 9. Exemplars from the CAFE Database </div>
			<img src="images/CAFE.jpeg" alt="Exemplars from the CAFE Database">
			&nbsp;<hr>&nbsp;


			<h2>Children Spontaneous Facial Expression Video Database (LIRIS-CSE) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://childrenfacialexpression.projet.liris.cnrs.fr/">Link</a>
			<h2>Description</h2>
			<p>A novel emotional database that contains movie clips/dynamic images of 12 ethnically diverse children. This unique database contains spontaneous/natural facial expression of children in diverse settings with diverse recording scenarios showing six universal or prototypic emotional expressions (happiness, sadness, anger, surprise, disgust and fear). Children are recorded in constraint free environment (no restriction on head movement, no restriction on hands movement, free sitting setting, no restriction of any sort) while they watched specially built/selected stimuli. This constraint free environment allowed the recording of spontaneous/natural expression of children as they occur.</p>
			<h2>Reference</h2>
			<p>Khan, R.A., Arthur, C., Meyer, A., Bouakaz, S. (2018). A novel database of Children's Spontaneous Facial Expressions (LIRIS-CSE). <em>Image and Vision Computing, Volumes 83-84</em>, March-April 2019. arXiv (2018) preprint, arXiv:1812.01555.</p>
			<h2>Contact</h2>
			<p><a href="https://childrenfacialexpression.projet.liris.cnrs.fr/site/requestnew">Request Form</a></p>
			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 10. Exemplars from the CSFE Video  Database</div>
			<img src="images/CSFE.png" alt="Exemplars from the CSFE Video  Database">
			&nbsp;<hr>&nbsp;


			<h2>City Infant Faces Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://docs.google.com/document/d/1_22mp0DXlw-lE9lUplKp_-LrTnD3w_xugfmHAEmaiec/edit">Link</a>
			<h2>Description</h2>
			<p>This database contains 60 photographs of positive infant faces, 54 photographs of negative infant faces, and 40 photographs of neutral infant faces. The images have high criterion validity and good test&ndash;retest reliability.</p>

			<h2>Reference</h2>
			<p>Webb, R., Ayers, S. &amp; Endress, A. The City Infant Faces Database: A validated set of infant facial expressions. Behav Res 50, 151&ndash;159 (2018). https://doi.org/10.3758/s13428-017-0859-9</p>

			<h2>Contact</h2>
			<p><a href="https://docs.google.com/document/d/1_22mp0DXlw-lE9lUplKp_-LrTnD3w_xugfmHAEmaiec/edit">Rebecca Webb</a></p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 11. Exemplars from the City Infant Faces Database</div>
			<img src="images/CITY_infant.png" alt="Exemplars from the City Infant Faces Database">
			&nbsp;<hr>&nbsp;


			<h2>CMU Multi-PIE face database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html">Link</a>
			<h2>Description</h2>
			<p>CMU Multi-PIE face database contains more than 750,000 images of 337 people recorded in up to four sessions over the span of five months. Subjects were imaged under 15 viewpoints and 19 illumination conditions while displaying a range of facial expressions.</p>

			<h2>Reference</h2>
			<p>Sim, T., Baker, S., &amp; Bsat, M. (2001). The CMU pose, illumination and expression database of human faces. Carnegie Mellon University Technical Report CMU-RI-TR-OI-02.</p>

			<h2>Contact</h2>
			<p>Ralph Gross, ralph@multiple.org</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 12. Exemplars from the CMU Database </div>
			<img src="images/CMU.png" alt="Exemplars from the CMU Database">
			&nbsp;<hr>&nbsp;

			<h2>Cohn-Kanade AU-Coded Facial Expression Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://paperswithcode.com/dataset/ck">Link</a>
			<h2>Description</h2>
			<p>The Cohn-Kanade AU-Coded Facial Expression Database affords a test bed for research in automatic facial image analysis and is available for use by the research community. Image data consist of approximately 500 image sequences from 100 subjects. Accompanying meta-data include annotation of FACS action units and emotion-specified expressions. Subjects range in age from 18 to 30 years. Sixty-five percent were female; 15 percent were African-American and three percent Asian or Latino.</p>
			<p>Subjects were instructed by an experimenter to perform a series of 23 facial displays that included single action units (e.g., AU 12, or lip corners pulled obliquely) and action unit combinations (e.g., AU 1+2, or inner and outer brows raised).  Each begins from a neutral or nearly neutral face.  For each, an experimenter described and modeled the target display.  Six were based on descriptions of prototypic emotions (i.e., joy, surprise, anger, fear, disgust, and sadness). </p>
			<h2>Reference</h2>
			<p>Kanade, T., Cohn, J. F., &amp; Tian, Y. (2000, March). Comprehensive database for facial expression analysis. In Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580) (pp. 46-53). IEEE.</p>

			<h2>Contact</h2>
			<p>Takeo Kanade, kanade@andrew.cmu.edu</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 13. Exemplars from the CK Database</div>
			<img src="images/CK.png" alt="Exemplars from the CK Database">
			&nbsp;<hr>&nbsp;


			<h2>Complex Emotion Expression Database (CEED) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0228248">Link</a>
			<h2>Description</h2>
			<p>The Complex Emotion Expression Database (CEED) , a digital stimulus set of 243 basic and 237 complex emotional facial expressions. The stimuli represent six basic expressions (angry, disgusted, fearful, happy, sad, and surprised) and nine complex expressions (affectionate, attracted, betrayed, brokenhearted, contemptuous, desirous, flirtatious, jealous, and lovesick) that were posed by Black and White formally trained, young adult actors.</p>

			<h2>Reference</h2>
			<p>Benda MS, Scherf KS (2020) The Complex Emotion Expression Database: A validated stimulus set of trained actors. PLoS ONE 15(2): e0228248. https://doi.org/10.1371/journal.pone.0228248</p>

			<h2>Contact</h2>
			<p></p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 14. Exemplars from the CEED Database</div>
			<img src="images/CEED.png" alt="Exemplars from the CEED Database">
			&nbsp;<hr>&nbsp;




			<h2>CVL Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://www.lrv.fri.uni-lj.si/facedb.html">Link</a>
			<h2>Description</h2>
			<p>The Computer Vision Laboratory Face Database contains photographs of 114 persons approximately 18 years of age, 7 images per person.</p>


			<h2>Reference</h2>
			<p>Mirage 2003, Conference on Computer Vision / Computer Graphics Collaboration for Model-based Imaging, Rendering, image Analysis and Graphical special Effects, March 10-11 2003, INRIA Rocquencourt, France, Wilfried Philips, Rocquencourt, INRIA, 2003, pp. 38-47.</p>

			<h2>Contact</h2>
			<p>peter.peer@fri.uni-lj.si; you will get a licence agreement to sign</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 15. Exemplars from the CVL Database</div>
			<img src="images/CVL.png" alt="Exemplars from the CVL Database">
			&nbsp;<hr>&nbsp;

			<h2 id="d">Dartmouth Database of Children's Faces  <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://lab.faceblind.org/k_dalrymple/ddcf">Link</a>
			<h2>Description</h2>
			<p>The Dartmouth Database of Children&#39;s Faces contains images of 40 male and 40 female models between the ages of 6 and 16. Models are photographed on a black background and are wearing black bibs and black hats to cover hair and ears. They are photographed from 5 different camera angles and pose 8 different facial expressions. Models were rated by independent raters and are ranked for the overall believability of their poses.</p>
			<h2>Reference</h2>
			<p>Dalrymple, K. A., Gomez, J., &amp; Duchaine, B. (2013). The Dartmouth Database of Children&rsquo;s Faces: Acquisition and validation of a new face stimulus set. PloS one, 8(11), e79131.</p>

			<h2>Contact</h2>
			<p>Kristen Dalrymple, kad@umn.edu</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 16. Exemplars from the Dartmouth Database</div>
			<img src="images/Dartmouth.jpeg" alt="Exemplars from the Dartmouth Database">
			&nbsp;<hr>&nbsp;

			<h2 id="f">Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://agingmind.utdallas.edu/download-stimuli/face-database/">Link</a>
			<h2>Description</h2>
			<p>The Face Database consists of 575 individual faces ranging from ages 18 to 93. Our database was developed to be more representative of age groups across the lifespan, with a special emphasis on recruiting older adults. The resulting database has faces of 218 adults age 18-29, 76 adults age 30-49, 123 adults age 50-69, and 158 adults age 70 and older.</p>

			<h2>Reference</h2>
			<p>Minear, M., &amp; Park, D. C. (2004). A lifespan database of adult facial stimuli. Behavior research methods, instruments, &amp; computers : a journal of the Psychonomic Society, Inc, 36(4), 630&ndash;633. https://doi.org/10.3758/bf03206543</p>

			<h2>Contact</h2>
			<p>parklab@utdallas.edu</p>

			<h2>Attribution</h2>
			<p>Faces may not be used for media events under any circumstances. If you publish a manuscript in a scientific journal that used the faces, please use the citation below</p>
			<!--<div class="figheading">Fig. </div>
			<img src="images/file" alt="Exemplars from the   Database">-->
			&nbsp;<hr>&nbsp;

			<h2>Face Image Meta-Database (fIMDb) &amp; ChatLab Facial Anomaly Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="https://osf.io/deszt/">Link</a>
			<h2>Description</h2>
			<h3>fIMDb</h3>
			<p>An index of face databases, their features, and how to access them has been unavailable. The &ldquo;Face Image Meta-Database&rdquo; (fIMDb) provides researchers with the tools to find the face images best suited to their research. The fIMDb is available from: <a href="https://cliffordworkman.com/resources/" target="_blank">https://cliffordworkman.com/resources/</a></p>
			<h3>ChatLab Facial Anomaly Database</h3>
			<p>The CFAD was developed to facilitate research on biases towards individuals with facial anomalies. The database allows searching by age, sex, ethnicity, pose, and type/etiology of anomaly. Results include the original stimuli, as well as images at various stages of pre-processing, e.g., normalized to interpupillary distance. </p>

			<h2>Reference</h2>
			<p>Workman, C. I., &amp; Chatterjee, A. (2020, June 24). The Face Image Meta-Database (fIMDb) &amp; ChatLab Facial Anomaly Database (CFAD): Tools for research on face perception and social stigma. https://doi.org/10.1016/j.metip.2021.100063</p>

			<h2>Contact</h2>
			<p></p>

			<h2>Attribution</h2>
			<p>If you are planning to publish research that used the CFAD stimuli, please cite us</p>
			<div class="figheading">Fig 17. Exemplar from the CFAD  Database"</div>
			<img src="images/CFAD.png" alt="Exemplar from the CFAD  Database">
			&nbsp;<hr>&nbsp;


			<h2>Face Place(s) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://sites.google.com/andrew.cmu.edu/tarrlab/resources/tarrlab-stimuli?authuser=0">Link</a>
			<h2>Description</h2>
			<p>This dataset includes multiple photographs for over 200 individuals of many different races with consistent lighting, multiple views, real emotions, and disguises (and some participants returned for a second session several weeks later with a haircut, or a new beard, etc.). The images are in jpeg format, 250x250 72 dpi 24 bit color.</p>

			<h2>Reference</h2>
			<p>Righi, G, Peissig, JJ, &amp; Tarr, MJ (2012) Recognizing disguised faces. Visual Cognition, 20(2), 143-169. doi:10.1080/13506285.2012.654624</p>

			<h2>Contact</h2>
			<p>Tarr Lab, Carnegie Mellon University, tarrlab@gmail.com</p>

			<h2>Attribution</h2>
			<p>If you use any of these images in publicly available work - talks, papers, etc. - you must acknowledge their source and adhere to the <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.  You must also include the following: Face images courtesy of Michael J. Tarr, Carnegie Mellon University, http://www.tarrlab.org/. Funding provided by NSF award 0339122.</p>
			<div class="figheading">Fig 18. Sample from dataset</div>
			<img src="images/600px-face-place.png" alt="Tarr Lab Face Place image showing four faces used in behavioural research">
			&nbsp;<hr>&nbsp;

			<h2>Face Recognition Technology (FERET) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://www.nist.gov/programs-projects/face-recognition-technology-feret">Link</a>
			<h2>Description</h2>
			<h3>FERET</h3>
			<p>The FERET database was collected in 15 sessions between August 1993 and July 1996. The database contains 1564 sets of images for a total of 14,126 images that includes 1199 individuals and 365 duplicate sets of images. A duplicate set is a second set of images of a person already in the database and was usually taken on a different day.</p>
			<h3>Color FERET</h3>
			<p>As part of the FERET program, a <a href="https://www.nist.gov/itl/products-and-services/color-feret-database">database of facial imagery</a> was collected between December 1993 and August 1996. The database is used to develop, test, and evaluate face recognition algorithms.
			</p>
			<h2>Reference</h2>
			<p>Phillips, P. J., Martin, A., Wilson, C. L., &amp; Przybocki, M. (2000). An introduction evaluating biometric systems. Computer, 33(2), 56-63.</p>

			<h2>Contact</h2>
			<p>P. Jonathon Phillips, jonathon.phillips@nist.gov</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 19. Exemplars from the colour FERET  Database</div>
			<img src="images/FERET.png" alt="Exemplars from the colour FERET  Database">
			&nbsp;<hr>&nbsp;

			<h2>Face Research Lab - London Set <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666">Link</a>
			<h2>Description</h2>
			<p>The London Set contains Images are of 102 adult faces 1350x1350 pixels in full color.</p>


			<h2>Reference</h2>
			<p>DeBruine, Lisa; Jones, Benedict (2017): Face Research Lab London Set. figshare. Dataset. https://doi.org/10.6084/m9.figshare.5047666.v5</p>
			<h2>Contact</h2>
			<p></p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 20. Exemplars from the London Set  Database</div>
			<img src="images/ResearchLabLondonSet.png" alt="Exemplars from the London Set  Database">
			&nbsp;<hr>&nbsp;

			<h2>Face Research Toolkit (FaReT) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="https://github.com/fsotoc/FaReT/tree/master/model-database/expressions">Link</a>
			<h2>Description</h2>
			<p>A free and open-source toolkit of three-dimensional models and software to study face perception. Contains 8 manipulatable facial expression models.</p>

			<h2>Reference</h2>
			<p>Hays, J. S., Wong, C., &amp; Soto, F. (2020). FaReT: A free and open-source toolkit of three-dimensional models and software to study face perception. Behavior Research Methods, 5.(6), 2604-2622.</p>
			<h2>Contact</h2>
			<p>Fabian Soto, Florida International University</p>

			<h2>Attribution</h2>
			<p></p>
			<div class="figheading">Fig 21. </div>
			<img src="images/FARET.gif" alt="Exemplar from the FaReT Database">
			&nbsp;<hr>&nbsp;
			<h2>FACES <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://faces.mpdl.mpg.de/imeji/">Link</a>
			<h3>FACES</h3>
			<p>FACES is a set of images of naturalistic faces of 171 young (n = 58), middle-aged (n = 56), and older (n = 57) women and men displaying each of six facial expressions: neutrality, sadness, disgust, fear, anger, and happiness. The database comprises two sets of pictures per person and per facial expression (a vs. b set), resulting in a total of 2,052 images.</p>


			<h3>Dynamic FACES</h3>

			<p>Dynamic FACES is an extension of the original FACES database. It is a database of morphed videos (n = 1,026) of young, middle-aged, and older adults displaying six naturalistic emotional facial expressions including neutrality, sadness, disgust, fear, anger, and happiness. Static images used for morphing came from the original FACES database. Videos were created by transitioning from a static neutral image to a target emotion. Videos are available in 384 x 480 pixels as .mp4 files or in original size of 1280 x1600 as .mov files.</p>

			<h3>Scrambled FACES</h3>
			<p>All 2,052 images from the original FACES database were scrambled using MATLAB. With the randblock function, original FACES files were treated as 800x1000x3 matrices &ndash; the third dimension denoting specific RGB values &ndash; and partitioned into non-overlapping 2x2x3 blocks. The matrices were then randomly shuffled by these smaller blocks, providing final images that matched the dimensions of the original image and were composed of the same individual pixels, although arranged differently. All scrambled images are 800x1000 jpeg files (96 dpi).</p>
			<h2>References</h2>
			<p>Ebner, N., Riediger, M., &amp; Lindenberger, U. (2010). FACES&mdash;A database of facial expressions in young, middle-aged, and older women and men: Development and validation. Behavior research Methods, 42, 351-362. doi:10.3758/BRM.42.1.351.</p>

			<p>Holland, C. A. C., Ebner, N. C., Lin, T., &amp; Samanez-Larkin, G. R. (2019). Emotion identification across adulthood using the Dynamic FACES database of emotional expressions in younger, middle aged, and older adults. Cognition and Emotion, 33, 245-257. doi:10.1080/02699931.2018.1445981.</p>

		<div class="figheading">Fig 22.Exemplar from the FACES Database </div>
			<img src="images/FACES.png" alt="Exemplar from the FACES Database">
			&nbsp;<hr>&nbsp;

			<h2>FaceScrub <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://www.cognitoforms.com/ADSC2/FaceScrubDatasetPasswordRequest">Link</a>
			<h2>Description</h2>
			<p>A dataset with a total of 106,863 face images* of male and female 530 celebrities, with about 200 images per person. As such, it is one of the largest public face databases.</p>

			<h2>Reference</h2>
			<p>H.-W. Ng, S. Winkler. A data-driven approach to cleaning large face datasets. Proc. IEEE International Conference on Image Processing (ICIP), Paris, France, Oct. 27-30, 2014.</p>

			<h2>Contact</h2>
			<p><a href="https://www.cognitoforms.com/ADSC2/FaceScrubDatasetPasswordRequest">Request Form</a></p>
			&nbsp;<hr>&nbsp;

			<h2>FAMED Face Database (Video) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://chrislongmore.co.uk/famed/">Link</a>
			<h2>Description</h2>
			<p>The Faces and Motion Exeter Database (FAMED) is a video database of 32 male actors for use in psychological research.  Each actor was filmed from two viewpoints (full-face and three-quarter) whilst they performed a series of facial motions including the telling of three jokes, a short conversation, six facial expressions (smiling, anger, fear, disgust, surprise and sadness) and rigid motion such as head rotation from left to right and up and down.  The actors performed all actions three times; once with no headgear, once wearing a swimming cap to hide hair cues and once whilst wearing a wig.</p>

			<h2>Reference</h2>
			<p>Longmore, C. A., &amp; Tree, J. J. (2013). Motion as a cue to face recognition: Evidence from congenital prosopagnosia. Neuropsychologia, 51, 864-875</p>

			<h2>Contact</h2>
			<p>Chris Longmore, chris.longmore@plymouth.ac.uk</p>
			<div class="figheading">Fig 23.Exemplar from the FACES Database </div>
			<img src="images/FAMED.png" alt="Exemplar from the FAMED Database">
			&nbsp;<hr>&nbsp;
			<h2>FEI Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://fei.edu.br/~cet/facedatabase.html">Link</a>
			<h2>Description</h2>
			<p>The FEI Face Database is a Brazilian face database that contains a set of face images taken between June 2005 and March 2006 at the Artificial Intelligence Laboratory of FEI in S&atilde;o Bernardo do Campo, S&atilde;o Paulo, Brazil. There are 14 images for each of 200 individuals, a total of 2800 images. All images are colourful and taken against a white homogenous background in an upright frontal position with profile rotation of up to about 180 degrees. Scale might vary about 10% and the original size of each image is 640x480 pixels. All faces are mainly represented by students and staff at FEI, between 19 and 40 years old with distinct appearance, hairstyle, and adorns.  The number of male and female subjects are exactly the same and equal to 100.</p>
			<h2>Reference</h2>
			<p></p>
			<h2>Contact</h2>
			<p>Carlos Eduardo Thomaz, cet@fei.edu.br</p>
			<div class="figheading">Fig 24.Exemplar from the FEI Face Database </div>
			<img src="images/FEIFaceDatabase.jpeg.png" alt="Exemplar from the FEI Database">
			&nbsp;<hr>&nbsp;

	
			<h2>FG-NET Database with Facial Expressions and Emotions <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="https://www.jade-hs.de/team/frank-wallhoff/databases/">Link</a>
			<h2>Description</h2>
			<p>An image database containing face images showing a number of subjects performing the six different basic emotions defined by Eckman &amp; Friesen. The database has been developed in an attempt to assist researchers who investigate the effects of different facial expressions.</p>

			<h2>Reference</h2>
			<p>Frank Wallhoff; Bjorn Schuller; Michael Hawellek; Gerhard Rigoll: Efficient Recognition of Authentic Dynamic Facial Expressions on the Feedtum Database IEEE ICME, page 493-496. IEEE Computer Society, (2006)</p>

			<h2>Contact</h2>
			<p>Frank Wallhoff, frank.wallhoff@jade-hs.de.</p>
<div class="figheading">Fig 25.Exemplar from the FEED Database </div>
			<img src="images/FEED.jpeg" alt="Exemplar from the FEED Database">
			&nbsp;<hr>&nbsp;
			<h2 id="g">Glasgow Unfamiliar Face Database (GUFD) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://www.facevar.com/glasgow-unfamiliar-face-database">Link</a>
			<h2>Description</h2>
			<p>This database contains three images of 303 identities (each taken using separate cameras), similarity data quantifying perceived similarity between any two identities and 20 images per identity that have been extracted from a video clip for the purpose of familiarisation. </p>

			<h2>Contact</h2>
			<p>Mike Burton, mike.burton@york.ac.uk</p>
			&nbsp;<hr>&nbsp;

			<h2 id="j">Japanese and Caucasian Faces (Emotion and Neutral) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://www.humintell.com/research-tools/">Link</a>:

			<h2>Description</h2>
			<h3>Japanese and Caucasian Facial Expressions of Emotion - JACFEE</h3>
			<p>Consists of 56 color photographs of 56 different individuals who each illustrate one of the seven basic facial expressions of emotion.</p>
			<p>Fee: $95</p>
			<div class="figheading">Fig 25.Exemplar from the JAFCEE Database </div>
			<img src="images/JAFCEE.png" alt="Exemplar from the JAFCEE Database">
			<h3>JACNeuf</h3>

			<p>Consists of 56 color photographs of the subjects found in the JACFEE collection showing neutral facial expressions.</p>
			<p>Fee: $95</p>
			<div class="figheading">Fig 26.Exemplar from the JACNeuf Database </div>
			<img src="images/JACNeuf.png" alt="Exemplar from the JACNeuf Database">
			&nbsp;<hr>&nbsp;

			<h2>Japanese Female Facial Expression (JAFFE) Dataset <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="https://doi.org/10.5281/zenodo.3451523">Link</a>
			<h2>Description</h2><p>Japanese Female Facial Expression (JAFFE) Dataset contains 213 images of 10 Japanese female expressers. </p>

			<h2>Reference</h2>
			<p>Lyons, Michael, Kamachi, Miyuki, &amp; Gyoba, Jiro. (1998). The Japanese Female Facial Expression (JAFFE) Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3451524</p>

			<h2>Contact</h2>
			<p>Michael Lyons, <a href="https://orcid.org/0000-0002-7426-0628">ORCID</a></p>
			<h2>Attribution</h2><p>The JAFFE images may be used for non-commercial scientific research under certain terms of use, which must be accepted to access the data. JAFFE cannot be provided for the following:</p>
<ul>
<li>homework </li>
<li>course projects</li>
<li>undergraduate projects</li>
<li>personal use</li></ul>
			&nbsp;<hr>&nbsp;

			<h2 id="k">Karolinska Directed Emotional Faces (KDEF) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="https://www.emotionlab.se/kdef/">Link</a>
			<h2>Description</h2>
			<p>The Karolinska Directed Emotional Faces (KDEF) is a set of totally 4900 pictures of human facial expressions of emotion. The set contains 70 individuals, each displaying 7 different emotional expressions, each expression being photographed (twice) from 5 different angles.</p>

			<h2>Reference</h2>
			<p>Goeleven, E., De Raedt, R., Leyman, L., &amp; Verschuere, B. (2008). The Karolinska directed emotional faces: a validation study. Cognition and emotion, 22(6), 1094-1118.</p>

			<h2>Contact</h2>
			<p><a href="https://www.emotionlab.se/kdef/">Emotion Lab at Karolinska Institutet</a></p>
			
<h2>Attribution</h2>
<p>The KDEF stimuli may be used without charge for non-commercial research purposes only. All and any (re-)distribution and publishing without the written consent of the copyright holders is forbidden. Copyright holder is Karolinska Institutet, Department of Clinical Neuroscience, Section of Psychology, Stockholm, Sweden.</p>
<div class="figheading">Fig 27. Exemplars from the KDEF  Database</div>
<img src="images/KDEF.png" alt="Exemplars from the KDEF  Database">
&nbsp;<hr>&nbsp;
			
			
			
			
			&nbsp;<hr>&nbsp;
			<h2 id="l">Labeled Faces in the Wild <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://vis-www.cs.umass.edu/lfw/">Link</a>
			<h2>Description</h2>
			<p>The Labeled Faces in the Wild is a database of face photographs designed for studying the problem of unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web.</p>
			<h2>Reference</h2>
			<p>Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. <a href="http://vis-www.cs.umass.edu/lfw/lfw.pdf">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</a>. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.</p>
			<h2>Contact</h2>
			<p>Gary Huang, gbhuang@cs.umass.edu</p>
			<div class="figheading">Fig 28. Exemplars from the LFW  Database</div>
<img src="images/LFW.jpg" alt="Exemplars from the LFW Database">
			&nbsp;<hr>&nbsp;
			
			
			<h2>Libor Spacek's Facial Images Databases <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://cmp.felk.cvut.cz/~spacelib/faces/">Link</a>
			<h2>Description</h2>
			<p>This database contains</p>

			<ul>
				<li>Total number of individuals: 395</li>
				<li>Number of images per individual: 20</li>
				<li>Total number of images: 7900</li>
				<li>Gender:  contains images of male and female subjects</li>
				<li>Race:  contains images of people of various racial origins</li>
				<li>Age Range:  the images are mainly of first year undergraduate  students, so the majority of indivuals are between 18-20 years old but some older individuals are also present.</li>
				<li>Glasses: Yes</li>
				<li>Beards: Yes</li>
				<li>Image format: 24bit colour JPEG</li>
				<li>Camera used: S-VHS camcorder</li>
				<li>Lighting: artificial, mixture of tungsten and fluorescent overhead</li>
			</ul>
			<h2>Reference</h2>
			<p>D. Hond, L. Spacek `Distinctive Descriptions for Face Processing', Proceedings of the 8th British Machine Vision Conference BMVC97, Colchester, England, pp. 320-329, September 1997</p>
			<h2>Contact</h2>
			<p></p>
			<h2>Attribution</h2>
			<p>Conditions of use: You may freely download this data for your own research purposes. You should publish any computer recognition results achieved on this data with due acknowledgement (my name and <a href="https://cmp.felk.cvut.cz/~spacelib/faces/">URL to this page</a>). There is also a related publication (with my PhD student D.Hond).</p>
			<div class="figheading">Fig 29. Exemplars from the Spacek Database</div>
<img src="images/Spacek.jpg" alt="Exemplars from the Spacek Database">
			&nbsp;<hr>&nbsp;


			<h2 id="m">Makeup Datasets <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="http://www.antitza.com/makeup-datasets.html">Link</a> <h2>Description</h2><p>This dataset comprises of four datasets of female face images assembled for studying the impact of makeup on face recognition.</p>
			<h3>YouTube Makeup (YMU)</h3>
			<p>151 subjects, specifically Caucasian females, from YouTube makeup tutorials, before and after the application of makeup. There are four shots per subject: two shots before the application of makeup and two shots after the application of makeup.</p>
						<h3>Virtual Makeup (VMU)</h3>
			<p>VMU (Virtual Makeup): face images of Caucasian female subjects in the FRGC repository (http://www.nist.gov/itl/iad/ig/frgc.cfm) were synthetically modified to simulate the application of makeup on 51 female Caucasian subjects.</p>
			<h3>Makeup Induced Face Spoofing (MIFS)</h3>
			<p>Dataset consisting of 107 makeup-transformations taken from random YouTube makeup video tutorials. Each subject is attempting to spoof a target identity (celebrity)</p>			
			<h2>References</h2>
			<p>Dantcheva, C. Chen, A. Ross, &quot;Can Facial Cosmetics Affect the Matching Accuracy of Face Recognition Systems?,&quot; Proc. of 5th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), (Washington DC, USA), September 2012.</p>
			<p>C. Chen, A. Dantcheva, A. Ross, &quot;Automatic Facial Makeup Detection with Application in Face Recognition,&quot; Proc. of 6th IAPR International Conference on Biometrics (ICB), (Madrid, Spain), June 2013.</p>
			<p>A. Dantcheva, C. Chen, A. Ross, &quot;Can Facial Cosmetics Affect the Matching Accuracy of Face Recognition Systems?,&quot; Proc. of 5th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), (Washington DC, USA), September 2012.</p>
			<p>C. Chen, A. Dantcheva, T. Swearingen, A. Ross, &quot;Spoofing Faces Using Makeup: An Investigative Study,&quot; Proc. of 3rd IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), (New Delhi, India), February 2017.</p>
			<div class="figheading">Fig 30. Exemplars from the  YMU Database</div>
<img src="images/YMU2.jpg" alt="Exemplars from the  YMU Database">
			<div class="figheading">Fig 31. Exemplars from the  VMUDatabase</div>
<img src="images/VMU.jpg" alt="Exemplars from the  YMU Database">
			<div class="figheading">Fig 32. Exemplars from the MIFS  Database</div>
<img src="images/MU_spoofig.png" alt="Exemplars from the MIFS  Database">
			
			
			&nbsp;<hr>&nbsp;
			
			<h2>Messiner African American and Caucasian Male Sets  <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://iilab.utep.edu/stimuli.htm">Link</a>
			<h2>Description</h2>
			<p> These sets contain stimuli for use in our studies on cross-racial face recognition and identification.  The sets are available by email request to Dr. Meissner for those seeking to conduct research on face identification.  Our stimuli currently include African American and Caucasian male faces in two poses (smiling w/ casual clothing and non-smiling with burgundy sweatshirt). </p>
			<h2>Reference</h2>
			<p>Reference: Meissner, C. A., Brigham, J. C., &amp; Butz, D. A. (2005). Memory for own and other race faces: A dual process approach. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 19(5), 545-567.</p>
			<h2>Contact</h2>
			<p>Christian Meissener cmeissner@utep.edu</p>


<h2>Attribution</h2>
<p>To request access to these materials, please email Dr. Meissner via the following  address cmeissner@utep.edu?subject=Face Stimuli Request
 </p>
<div class="figheading">Fig 33. Exemplars from the Messiner  Database</div>
<img src="images/Messiner.jpg" alt="Exemplars from the Messiner  Database"><img src="images/Messiner2.jpg" alt="Exemplars from the Messiner  Database">
&nbsp;<hr>&nbsp;

			<h2>Montreal Set of Facial Displays of Emotion (MSFDE) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

			<a class="button" target="_blank" href="http://www.psychophysiolab.com/en/download.php">Link</a>
			<h2>Description</h2>
			<p>MSFDE consists of emotional facial expressions by men and women of European, Asian, and African descent. Each expression was created using a directed facial action task and all expressions were FCAS coded to assure identical expressions across actors.</p>

			<p>The set contains expressions of happiness, sadness, anger, fear, disgust, and embarrassment as well as a neutral expression for each actor.</p>

			<h2>Contact</h2>
			<p>Social Psychophysiology Laboratory, Universit&eacute; du Qu&eacute;bec &agrave; Montr&eacute;al</p>
		<h2>Attribution</h2>
<p></p>
<div class="figheading">Fig 34. Exemplars from the MSFDE  Database</div>
<img src="images/MSFDE.jpg" alt="Exemplars from the MSFDE  Database">

			
			
			&nbsp;<hr>&nbsp;
			<h2>MMI Facial Expression Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://mmifacedb.eu/">Link</a>
			<h2>Description</h2>

			<p>The MMI Facial Expression Database is an ongoing project, that aims to deliver large volumes of visual data of facial expressions to the facial expression analysis community. The database consists of over 2900 videos and high-resolution still images of 75 subjects.</p>

			<h2>Reference</h2>
			<p>Valstar, M., &amp; Pantic, M. (2010, May). Induced disgust, happiness and surprise: an addition to the mmi facial expression database. In Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect (p. 65).</p>

			<h2>Contact</h2>
			<p><a href="https://mmifacedb.eu/accounts/register/">Request a user account</a></p>
			
					<h2>Attribution</h2>
<p></p>
<div class="figheading">Fig 35. Exemplars from the MMI  Database</div>
<img src="images/MMI.png" alt="Exemplars from the MMI  Database">

			
			
			&nbsp;<hr>&nbsp;
			<h2>MR2 Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://osf.io/skbq2/">Link</a>
			<h2>Description</h2>
			<p>The MR2 is a multi-racial, mega-resolution database of facial stimuli, created in collaboration with the psychologist Kurt Gray and the photographer Titus Brooks Heagins.  It contains 74 full-color images of men and women of European, African, and East Asian descent.</p>

			<h2>Reference</h2>
			<p>Strohminger, N., Gray, K., Chituc, V., Heffner, J., Schein, C., and Heagins, T.B. (in press). The MR2: A multi-racial mega-resolution database of facial stimuli. Behavior Research Methods.</p>

			<h2>Contact</h2>
			<p>Nina Strohminger, humean@wharton.upenn.edu</p>
<h2>Attribution</h2>
<p>The database is free to access, with the proviso that any publication or presentation using the database give proper attribution. The MR2 face database is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>
<div class="figheading">Fig 36. Exemplar from the MR2  Database</div>
<img src="images/MR2.png" alt="Exemplar from the MR2  Database">
			&nbsp;<hr>&nbsp;

			<h2>MUCT Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="http://www.milbo.org/muct/">Link</a>
			<h2>Description</h2>
			<p>The MUCT Face Database consists of 3755 faces with 76 manual landmarks. The database was created to provide more diversity of lighting, age, and ethnicity than currently available landmarked 2D face databases.</p>
			<h2>Reference</h2>
			<p>Milborrow, S., Morkel, J., &amp; Nicolls, F. (2010). <a href="http://www.milbo.org/muct/The-MUCT-Landmarked-Face-Database.pdf">The MUCT landmarked face database</a>. Pattern recognition association of South Africa, 201(0).</p>
			<h2>Contact</h2>
			<p>Stephen Milborrow, milbo@sonic.net</p>
			<div class="figheading">Fig 37. Exemplars from the MUCT  Database</div>
<img src="images/MUCT.png" alt="Exemplars from the MUCT  Database">
			
			
			
			&nbsp;<hr>&nbsp;
			<h2 id="n">NimStim Set of Facial Expressions <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
			<a class="button" target="_blank" href="https://osf.io/y86rw/wiki/home/">Link</a>
			<h2>Description</h2>
			<p>The NimStem Set of Facial Expressions is a broad dataset comprising of 672 images of naturally posed photographs by 43 professional actors (18 female, 25 male) ranging from 21 to 30 years old. Actors from a diverse sample were chosen to portray emotional expressions within this dataset. To be precise, the actors were African-American (N = 10), Asian-American (N = 6), European-American (N = 25), Latino-American (N = 2). The images contained in this dataset include eight emotional expressions, namely: neutral, angry, disgust, surprise, sad, calm, happy, and afraid. Both open and closed mouth versions were provided for all emotional expressions, with the exception of surprise (only open mouth provided) and happy (high arousal open mouth/exuberant provided).</p>

			<h2>Reference</h2>
				<p>Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., ... &amp; Nelson, C. (2009). The NimStim set of facial expressions: judgments from untrained research participants. Psychiatry Research, 168(3), 242-249.</p>
				<h2>Contact</h2>
				<p>Please direct questions and comments to admin@macbrain.org</p>
				&nbsp;<hr>&nbsp;

				<h2 id="o">Oslo Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="https://sirileknes.com/oslo-face-database/">Link</a>
				<h2>Description</h2>
				<p>The Oslo Face Database consists of ~200 male and female faces of neutral expression with three gaze directions: left, center and right. The photos were taken in 2012 of students from the University of Oslo.</p>
				
				&nbsp;<hr>&nbsp;

				<h2>Oulu-CASIA NIR&amp;VIS Facial Expression Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

				<a class="button" target="_blank" href="https://www.oulu.fi/cmvs/node/41316">Link</a>
				<h2>Description</h2>
				<p>This set contains videos with the six typical expressions (happiness, sadness, surprise, anger, fear, disgust) from 80 subjects captured with two imaging systems, NIR (Near Infrared) and VIS (Visible light), under three different illumination conditions: normal indoor illumination, weak illumination (only computer display is on) and dark illumination (all lights are off).</p>

				<h2>Reference</h2>
				<p>Zhao, G., Huang, X., Taini, M., Li, S. Z., &amp; Pietik&auml;Inen, M. (2011). Facial expression recognition from near-infrared videos. <i>Image and Vision Computing</i>, <i>29</i>(9), 607-619.</p>

				<h2>Contact</h2>
				<p>Guoying Zhao, guoying.zhao@oulu.fi</p>
				&nbsp;<hr>&nbsp;
				
				<h2>Psychological Image Collection at Sterling (PICS) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<h2>Description</h2>
				<p>The Psychogical Image Collection at Stirling (PICS) contains two databases of face images. </p>

				<a class="button" target="_blank" href="http://pics.psych.stir.ac.uk/ESRC/index.htm">Link</a>
&nbsp;<ul><li>45 male and 54 female sets</li></ul>

				<a class="button" target="_blank" href="http://pics.psych.stir.ac.uk/2D_face_sets.htm">Link</a>

				&nbsp;<ul>
					<li>9 collections containing hundreds of images</li>
				</ul>
				<p>Reference: varies</p>

				<h2>Contact</h2>
				<p>Peter Hancock, pjbh1@stir.ac.uk</p>
							<div class="figheading">Fig 38. Exemplars from the PICS  Database</div>
<img src="images/Stirling.png" alt="Exemplars from the PICS  Database">

				&nbsp;<hr>&nbsp;

				<h2 id="r">Radboud Faces Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="https://rafd.socsci.ru.nl/RaFD2/RaFD?p=main">Link</a>
				<h2>Description</h2>
				<p>The Radboud Faces Database (RaFD)is a set of pictures of 67 models (including Caucasian males and females, Caucasian children, both boys and girls, and Moroccan Dutch males) displaying 8 emotional expressions. The RaFD in an initiative of the <a href="https://www.ru.nl/bsi/">Behavioural Science Institute</a> of the Radboud University Nijmegen, which is located in Nijmegen (the Netherlands), and can be used freely for non-commercial scientific research by researchers who work for an officially accredited university.</p>
				<h2>Reference</h2>
				<p>Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., Hawk, S.T., &amp; van Knippenberg, A. (2010). Presentation and validation of the Radboud Faces Database. <em>Cognition &amp; Emotion, 24</em>(8), 1377—1388. DOI: 10.1080/02699930903485076</p>
				<h2>Contact</h2>
				<p>info@rafd.nl</p>
				<div class="figheading">Fig 39. Exemplars from the RaFD  Database</div>
<img src="images/RaFD.png" alt="Exemplars from the RaFD  Database">
				
				
				&nbsp;<hr>&nbsp;
				<h2>RADIATE Emotional Face Stimulus Set <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

				<a class="button" target="_blank" href="http://fablab.yale.edu/page/assays-tools">Link</a>
				<h2>Description</h2>
				<p>Radiate is an open-access face stimulus set of 1721 racially diverse expressions is described. Sixteen different emotions in color and in black and white versions are included.</p>

				<h2>Reference</h2>
				<p>Conley, M. I., Dellarco, D. V., Rubien-Thomas, E., Cohen, A. O., Cervera, A., Tottenham, N., &amp; Casey, B. J. (2018). The racially diverse affective expression (RADIATE) face stimulus set. Psychiatry research.</p>
				
							
				&nbsp;<hr>&nbsp;

				<h2 id="s">Sheffield Face Database <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

				<a class="button" target="_blank" href="https://www.visioneng.org.uk/datasets/">Link </a>
				<h2>Description</h2>
				<p>The Sheffield Database (previously UMIST) consists of 564 images of 20 individuals (mixed race/gender/appearance). Each individual is shown in a range of poses from profile to frontal views &ndash; each in a separate directory labelled 1a, 1b, &hellip; 1t and images are numbered consecutively as they were taken. The files are all in PGM format, approximately 220 x 220 pixels with 256-bit grey-scale.</p>

				<h2>Reference</h2>
				<p>Wechsler, H., Phillips, J. P., Bruce, V., Soulie, F. F., &amp; Huang, T. S. (Eds.). (2012). Face recognition: From theory to applications (Vol. 163). Springer Science &amp; Business Media.</p>

				<h2>Contact</h2>
				<p><a href="https://www.visioneng.org.uk/datasets/">Laboratory of Vision Engineering (LoVE)</a>, University of Lincoln</p>
				<h2>Attribution</h2>
				<p>The authors grant the right to use the face database with the following restrictions:</p>
<ul><li>Only images of individuals 1a and 1e may be published (and then only with permission). This is not out of vanity, but for legal reasons.</li><li>Acknowledgement of use of this database should be provided in any publication. We would, of course, be very interested to hear about any publications.</li></ul>

<div class="figheading">Fig 40. Exemplars from the Sheffield  Database</div>
<img src="images/Sheffield.gif" alt="Exemplars from the Sheffield  Database">				
				&nbsp;<hr>&nbsp;

				<h2 id="t">Todorov Synthetic Faces Databases <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="https://tlab.uchicago.edu/databases/">Link </a>
				<h2>Description</h2>
				<p>A compendium of computer-generated synthetic faces.</p>

				<p>References: varies</p>

				<h2>Contact</h2>
				<p><a href="https://www.chicagobooth.edu/faculty/directory/t/alexander-todorov">Alexander Todorov</a>, University of Chicago</p>

				<h3>Database 1</h3>

				<p>300 randomly generated faces parametrically manipulated to vary on their perceived value on social dimensions such as trustworthiness and dominance. These faces were generated by data-driven computational models.</p>


				<h3>Database 2</h3>

				<p>525 faces manipulated on face shape: 25 (face identities) x 3 (trait dimensions: perceived dominance, threat, and trustworthiness) x 7 (parametric face manipulations, ranging from -3 to +3SD with a step of 1SD).</p>


				<h3>Database 3</h3>

				<p>490 faces manipulated on face shape and orthogonally on perceived trustworthiness and dominance: 10 (face identities) x 7 (parametric face manipulations on perceived dominance, ranging from -3 to +3SD with a step of 1SD) x 7 (parametric face manipulations on perceived trustworthiness, ranging from -3 to +3SD with a step of 1SD).</p>


				<h3>Database 4</h3>

				<p>3,675 faces manipulated on face shape and reflectance: 25 (face identities) x 7 (trait dimensions: perceived attractiveness, competence, dominance, extroversion, likability, threat, and trustworthiness) x 7 (parametric face manipulations, ranging from -3 to +3SD with a step of 1SD) x 3 (face race: Asian, Black, White). </p>


				<h3>Database 5</h3>

				<p>13,125 faces manipulated on face shape and reflectance: 25 (face identities) x 7 (trait dimensions: perceived attractiveness, competence, dominance, extroversion, likability, threat, and trustworthiness) x 25 (parametric face manipulations, ranging from -3 to +3SD with a step of 0.25SD) x 3 (face race: Asian, Black, White). </p>


				<h3>Database 6</h3>

				<p>4,000 faces used to build a model of attractiveness. Text files, data files, and python and Matlab scripts are also included</p>


				<h3>Database 7</h3>
				<p>1,400 faces manipulated on face shape and reflectance by gender-specific models built by Oh, Dotsch, Porter, &amp; Todorov (2020): 25 (face identities) x 2 (gender models: for males and females) x 2 (trait dimensions: perceived dominance and trustworthiness) x 7 (parametric face manipulations, ranging from -3 to +3SD with a step of 1SD) x 2 (face gender: male and female).</p>

				<h3>Database 8</h3>

				<p>350 faces manipulated on perceived competence controlling for attractiveness: 25 (face identities) x 7 (parametric face manipulations, ranging from -3 to +3SD with a step of 1SD) x 2 (models: attractiveness-subtracted and attractiveness-orthogonal). </p>
				
				
				
				<h2>Attribution</h2>
				<p>The databases listed above are freely available to researchers who intend to conduct non-profit, academic research. Researchers who download the databases should use the stimuli for non-profit research only and should acknowledge the proper sources of the stimuli and any references relevant to the data set.</p>

&nbsp;<hr>&nbsp;

				<h2 id="u">UB KinFace <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>

				<a class="button" target="_blank" href="http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm">Link</a>
				<h2>Description</h2>
				<p>UB KinFace database is used to develop, test, and evaluate kinship verification and recognition algorithms. It comprises 600 images of 400 people which can be separated into 200 groups. Each group is composed of child, young parent and old parent images. Most of images in the database are real-world collections of public figures (celebrities and politicians) from Internet. To the best of our knowledge, it is the first database that contains all children, young parents and old parents for the purpose of kinship verification.</p>
				
				<h2>Reference</h2>
				<p> Ming Shao, Siyu Xia and Yun Fu, &ldquo;Genealogical Face Recognition based on UB KinFace Database,&rdquo; <i>IEEE CVPR Workshop on Biometrics</i> (BIOM), 2011.</p>
				<h2>Contact</h2>
				<p>Yun Raymond Fu, yunfu@ece.neu.edu</p>
				<h2>Attribution</h2>
				<p>This dataset is for non-commercial research purposes only. The image copyright belongs to the original author or the media as listed in the following URL file. If you find this collection useful for your research, please cite the paper above.</p>
				&nbsp;<hr>&nbsp;

				<h2>US Politicians <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="https://tlab.uchicago.edu/databases/">Link</a>
				<h2>Description</h2>
				<p>This database contains 550 photos of US politicians who competed either in a gubernatorial race (248) or in a house race (302). The database also contains the politicians&rsquo; perceived competence from their photos, as measured in a forced choice competence judgement of participants unfamiliar with the politicians. As such, these judgments simply indicate perceptions and are in no way indicative of the actual competence of the politicians.</p>

				<h2>Contact</h2>
				<p><a href="https://www.chicagobooth.edu/faculty/directory/t/alexander-todorov">Alexander Todorov</a>, University of Chicago</p>
<h2>Attribution</h2>
<p>The database listed is freely available to researchers who intend to conduct non-profit, academic research. Researchers who download the databases should use the stimuli for non-profit research only and should acknowledge the proper sources of the stimuli and any references relevant to the data set.</p>
				&nbsp;<hr>&nbsp;

				<h2 id="y">Yale Face Database A <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="http://vision.ucsd.edu/content/yale-face-database">Link</a>
				<h2>Description</h2>
				<h3>Yale Face Database A</h3>
				<p>The Yale Face Database contains 165 grayscale images in GIF format of 15 individuals. There are 11 images per subject, one per different facial expression or configuration: center-light, w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink.</p>
				<h2>Yale Face Database B <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="http://vision.ucsd.edu/~iskwak/ExtYaleDatabase/Yale%20Face%20Database.htm">Link</a>
				<h2>Description</h2>
				<h3>Yale Face Database B</h3>
				<p>The Yale Face Database B (1GB) contains 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions).</p>

				<h2>Reference</h2>

				<h2>Contact</h2>
				<p><a href="http://vision.ucsd.edu/people">UCSD Computer Vision</a> </p>
				
				<h2>Attribution</h2>
				<p>It is free to use the data for research purposes.
If experimental results are obtained that use images from within the database, all publications of these results should acknowledge the use of the "Yale Face Database".</p>
				
				
				&nbsp;<hr>&nbsp;
				<h2>Yonsei Face Database (YFace DB) <a href="#top"><img src="../images/top.png" width="20" alt="up arrow to click to go back to the top"></a></h2>
				<a class="button" target="_blank" href="https://dmhen.yonsei.ac.kr/yface-db.html">Link</a>
				<h2>Description</h2>
				<p>The Yonsei Face Database (YFace DB), consists of both static and dynamic face stimuli for six basic emotions (happiness, sadness, anger, surprise, fear, and disgust), and to test its validity. The database includes selected pictures (static stimuli) and film clips (dynamic stimuli) of 74 models (50% female) aged between 19 and 40.</p>

				<h2>Reference</h2>
				<p>Chung, K. M, Kim, S.J., Jung, W. H., &amp; Kim, V. Y. (2019). Development and Validation of the Yonsei Face Database (Yface DB). Frontiers in Psychology, 10, 2626. https://doi.org/10.3389/fpsyg.2019.02626</p>
				
				<h2>Attribution</h2>
				<p>Only a PHD-holding faculty at a non-profit, degree-granting, academic institution or a representative of an affiliation colud request for the use of Y-face DB.</p>
				&nbsp;<hr>&nbsp;


				<!--template <h2>Title</h2>
<a class="button" target="_blank" href="link">Link</a> 
<h2>Description</h2>
<p></p>

<h2>Reference</h2>
<p></p>

<h2>Contact</h2>
<p></p>

<h2>Attribution</h2>
<p></p>
<div class="figheading">Fig. </div>
<img src="images/file" alt="Exemplars from the   Database">
&nbsp;<hr>&nbsp;	-->
				<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
			       </section>
      <footer>
            <p>This project is maintained by <a href="https://github.com/GlenPennington">Dr Glen Pennington</a></p>			<p id="demo"</p>
			<script>
let text = document.lastModified;
document.getElementById("demo").innerHTML = text;
</script>
                  </footer>
    </div>

  </body>
</html>
