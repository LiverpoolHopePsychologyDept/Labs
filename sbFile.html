<!DOCTYPE html>
<html lang="en">
  <head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HNN5E2KR1W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HNN5E2KR1W');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="minimal.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
	  <title>Liverpool Hope Department of Psychology Labs and Resources</title>
  </head>
  <body>
    <table border="1">
<tbody>
<tr>
	<th>Name</th>
	<th>Reference</th>
	<th>Link</th>
</tr>
<tr>
<th rowspan="7">100000 humans that don't exist</th>
<td>NK</td>
<td><a href="https://generated.photos/humans">URL</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Stimulus Type</th>
</tr>
<tr>
<td>NS</td>
<td>Body</td>
</tr>
<tr>
<th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">AI generated human whole body images</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>NA</td>
<td>NS</td>
</tr>

<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">AI generated faces</th>
<td>NK</td>
<td><a href="https://generated.photos/faces">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">AI generated human faces</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>NA</td>
<td>NK</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Glasgow Unfamiliar Face Database</th>
<td><a href="http://www.facevar.com/glasgow-unfamiliar-face-database">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Database of unfamiliar faces</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>Mike Burton, mike.burton@york.ac.uk</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">10k US Adult Faces</th>
<td>Bainbridge, W.A., Isola, P., &amp; Oliva, A. (2013). The intrinsic memorability of face images. Journal of Experimental Psychology: General. Journal of Experimental Psychology: General, 142(4), 1323-1334&nbsp;</td>
<td><a href="http://wilmabainbridge.com/facememorability2.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">10,168 natural face photographs and several measures for 2,222 of the faces, including memorability scores, computer vision and psychology attributes, and landmark point annotations.</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NK</td>
<td>NK</td>
</tr>


<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">American Multiracial Face Database</th>
<td>Chen, J. M., Norman, J. B., &amp; Nam, Y. (2021). Broadening the stimulus set: introducing the American multiracial faces database. Behavior Research Methods,&nbsp;53(1), 371-389&nbsp;</td>
<td><a href="https://osf.io/qsdrp/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">110 faces, smiling/neutral</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>https://jacquelinemchen.wixsite.com/sciplab/face-database</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Amsterdam Dynamic Facial Expression Set</th>
<td>Van der Schalk, J., Hawk, S. T., Fischer, A. H., &amp; Doosje, B. J. (in press).Moving faces, looking places: The Amsterdam Dynamic Facial Expressions Set (ADFES), Emotion&nbsp;</td>
<td><a href="https://aice.uva.nl/research-tools/adfes-stimulus-set/adfes-stimulus-set.html?cb">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">648 dynamic filmed emotional expressions, enjoyment, surprise, fear, sadness, anger, disgust, neutral, contempt, pride, embarrassmenr</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td><a href="https://aice.uva.nl/research-tools/adfes-stimulus-set/request-for-use/request-for-use.html">Request for use</a></td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Warsaw Set of Emotional Facial Expressions</th>
<td>Olszanowski, M., Pochwatko, G., Kukliński, K., Ścibor-Rylski, M., Lewicki, P., Ohme, R. (2015). Warsaw set of emotional facial expression pictures: a validation study of facial display photographs. Frontiers in Psychology, 5:1516. doi: 10.3389/fpsyg.2014.01516&nbsp;</td>
<td><a href="http://www.emotional-face.org/wsefep">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">210 photos emotions enjoyment, surprise, fear, sadness, anger, disgust, neutral</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>NS</td>
<td><a href="http://wzzeet.website.pl/wordpress/?page_id=61">Link to access</a></td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">AT&amp;T Database of Faces</th>
<td>Samaria, F. S. (1994). Face recognition using hidden Markov models (Doctoral dissertation, University of Cambridge).&nbsp;</td>
<td><a href="https://git-disl.github.io/GTDLBench/datasets/att_face_dataset/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">400 face images, greyscale</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>NS</td>
<td>NS</td>
</tr>

<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Basel Face Database</th>
<td>Walker, M., Sch&ouml;nborn, S., Greifeneder, R., &amp; Vetter, T. (2018). The Basel Face Database: A validated set of photographs reflecting systematic differences in Big Two and Big Five personality dimensions. PloS one, 13(3). doi: https://doi.org/10.1371/journal.pone.0193190&nbsp;</td>
<td><a href="https://bfd.unibas.ch/en/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">600 images representing personality dimensions from the Big Two and Big Five</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Bogazici Face Database</th>
<td>Saribay SA, Biten AF, Meral EO, Aldan P, Třebick&yacute; V, Kleisner K (2018) The Bogazici face database: Standardized photographs of Turkish faces with supporting materials. PLoS ONE 13(2): e0192018. https://doi.org/10.1371/journal.pone.0192018&nbsp;</td>
<td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192018">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Turkish undergraduate faces</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Chicago Face Database</th>
<td>Ma, Correll, &amp; Wittenbrink (2015). The Chicago Face Database: A Free Stimulus Set of Faces and Norming Data.&nbsp;Behavior Research Methods, 47, 1122-1135.&nbsp;https://doi.org/10.3758/s13428-014-0532-5&nbsp;</td>
<td><a href="https://www.chicagofaces.org/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Faces of people between 17-65</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>bernd.wittenbrink@chicagobooth.edu</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Chicago Face Database - Multiracial</th>
<td>Ma, Kantner, &amp; Wittenbrink, (2020). Chicago Face Database: Multiracial Expansion. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01482-5.&nbsp;</td>
<td><a href="https://www.chicagofaces.org/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Extension of the CFD</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>bernd.wittenbrink@chicagobooth.edu</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Chicago Database- India</th>
<td>Lakshmi, Wittenbrink, Correll, &amp; Ma (2020). The India Face Set: International and Cultural Boundaries Impact Face Impressions and Perceptions of Category Membership. Frontiers in Psychology, 12, 161. https://doi.org/10.3389/fpsyg.2021.62767&nbsp;</td>
<td><a href="https://www.chicagofaces.org/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Extension of the CFD</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>bernd.wittenbrink@chicagobooth.edu</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Child Affective Facial Expression Set</th>
<td>LoBue, V. &amp; Thrasher, C. (2015). The Child Affective Facial Expression (CAFE) Set: Validity and reliability from untrained adults.&nbsp;Frontiers in Emotion Science, 5.&nbsp;PDF&nbsp;</td>
<td><a href="https://www.childstudycenter-rutgers.com/the-child-affective-facial-expression-se">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">1200 images aged 2-8 Childrens facial expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Children Spontaneous Facial Expression Video Database</th>
<td>A novel database of Children's Spontaneous Facial Expressions (LIRIS-CSE). Rizwan Ahmed Khan, Crenn Arthur, Alexandre Meyer, Saida Bouakaz. Image and Vision Computing, Volumes 83&ndash;84, March&ndash;April 2019. arXiv (2018) preprint, arXiv:1812.01555.&nbsp;</td>
<td><a href="https://childrenfacialexpression.projet.liris.cnrs.fr/site/requestnew">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Dynamic clips with prototypic emotions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>NS</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">City Infant Faces Database</th>
<td>Webb, R., Ayers, S. &amp; Endress, A. The City Infant Faces Database: A validated set of infant facial expressions. Behav Res 50, 151&ndash;159 (2018). https://doi.org/10.3758/s13428-017-0859-9&nbsp;</td>
<td><a href="https://docs.google.com/document/d/1_22mp0DXlw-lE9lUplKp_-LrTnD3w_xugfmHAEmaiec/edit">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>NS</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">154 infant faces positive/negative/neutral</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">CMU Multi-PIE Face Database</th>
<td>Sim, T., Baker, S., &amp; Bsat, M. (2001). The CMU pose, illumination and expression database of human faces. Carnegie Mellon University Technical Report CMU-RI-TR-OI-02&nbsp;</td>
<td><a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">75000 images of 337 people</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>NS</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Complex Emotion Expression Database (CEED)</th>
<td>Benda MS, Scherf KS (2020) The Complex Emotion Expression Database: A validated stimulus set of trained actors. PLoS ONE 15(2): e0228248. https://doi.org/10.1371/journal.pone.0228248&nbsp;</td>
<td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0228248">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Expressions (angry, disgusted, fearful, happy, sad, and surprised) and nine complex expressions (affectionate, attracted, betrayed, brokenhearted, contemptuous, desirous, flirtatious, jealous, and lovesick)</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>NS</td>
</tr>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Computer Vision Laboratory Face Database</th>
<td>Mirage 2003, Conference on Computer Vision / Computer Graphics Collaboration for Model-based Imaging, Rendering, image Analysis and Graphical special Effects, March 10-11 2003, INRIA Rocquencourt, France, Wilfried Philips, Rocquencourt, INRIA, 2003, pp. 38-47.&nbsp;</td>
<td><a href="http://www.lrv.fri.uni-lj.si/facedb.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Front and side views of faces</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Dartmouth Database of Children's Faces</th>
<td>Dalrymple, K. A., Gomez, J., &amp; Duchaine, B. (2013). The Dartmouth Database of Children&rsquo;s Faces: Acquisition and validation of a new face stimulus set. PloS one, 8(11), e79131&nbsp;</td>
<td><a href="https://lab.faceblind.org/k_dalrymple/ddcf">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Children's faces with hair and clothing obscured</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>Yes</td>
<td>kad@umn.edu</td>
</tr>

<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Face Database</th>
<td>Minear, M., &amp; Park, D. C. (2004). A lifespan database of adult facial stimuli. Behavior research methods, instruments, &amp; computers : a journal of the Psychonomic Society, Inc, 36(4), 630&ndash;633. https://doi.org/10.3758/bf03206543&nbsp;</td>
<td><a href="https://agingmind.utdallas.edu/download-stimuli/face-database/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr><td colspan="2">NS</td>
</tr>

<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>NS</td>
<td>parklab@utdallas.edu</td>
</tr>


<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">ChatLab Facial Anomaly Database</th>
<td>Workman, C. I. &amp; Chatterjee, A. (2021). The Face Image Meta-Database (fIMDb) &amp; ChatLab Facial Anomaly Database (CFAD): Tools for research on face perception and social stigma. Methods in Psychology, 5(100063):1-9.&nbsp;</td>
<td><a href="https://clffwrkmn.net/cfad/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Faces with anomalies</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>NS</td>
<td>NS</td>
</tr>

<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Face Recognition Technology</th>
<td>Phillips, P. J., Martin, A., Wilson, C. L., &amp; Przybocki, M. (2000). An introduction evaluating biometric systems. Computer, 33(2), 56-63&nbsp;</td>
<td><a href="https://www.nist.gov/programs-projects/face-recognition-technology-feret">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td>14,126 images that includes 1199 individuals</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>P. Jonathon Phillips, jonathon.phillips@nist.gov</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Face Research Lab - London Set</th>
<td>DeBruine, Lisa; Jones, Benedict (2017): Face Research Lab London Set. figshare. Dataset. https://doi.org/10.6084/m9.figshare.5047666.v5</td>
<td><a href="https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">102 adult faces 1350x1350 pixels in full color</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Faces</th>
<td>Ebner, N., Riediger, M., &amp; Lindenberger, U. (2010). FACES&mdash;A database of facial expressions in young, middle-aged, and older women and men: Development and validation.&nbsp;Behavior research Methods, 42, 351-362. https://doi.org/10.3758/BRM.42.1.351.&nbsp;</td>
<td><a href="https://faces.mpdl.mpg.de/imeji/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">172 individuals with six facial expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Dynamic Faces</th>
<td>Holland, C. A. C., Ebner, N. C., Lin, T., &amp; Samanez-Larkin, G. R. (2019). Emotion identification across adulthood using the Dynamic FACES database of emotional expressions in younger, middle aged, and older adults.&nbsp;Cognition and Emotion, 33, 245-257. https://doi.org/10.1080/02699931.2018.1445981.&nbsp;</td>
<td><a href="https://faces.mpdl.mpg.de/imeji/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr><td>NS</td>
<td>NS</td>
</tr>

<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">1026 morphed videos six facial expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Faces and Motion Exeter Database</th>
<td>Longmore, C. A., &amp; Tree, J. J. (2013). Motion as a cue to face recognition: Evidence from congenital prosopagnosia. Neuropsychologia, 51, 864-875&nbsp;</td>
<td><a href="https://chrislongmore.co.uk/famed/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td>32 male actors, filmed from two viewpoints</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Chris Longmore,&nbsp;chris.longmore@plymouth.ac.uk</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">FEI Face Database</th>
<td><a href="https://fei.edu.br/~cet/facedatabase.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Brazilian face database 200 individuals 2800 images</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Carlos Eduardo Thomaz,&nbsp;cet@fei.edu.br</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">FG-NET Database with Facial Expressions and Emotions</th>
<td>Frank Wallhoff; Bjorn Schuller; Michael Hawellek; Gerhard Rigoll: Efficient Recognition of Authentic Dynamic Facial Expressions on the Feedtum Database IEEE ICME, page 493-496. IEEE Computer Society, (2006)&nbsp;</td>
<td><a href="https://www.jade-hs.de/team/frank-wallhoff/databases/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>NS</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Six basic emotions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Japanese and Caucasian Facial Expressions of Emotion</th>
<td><a href="https://www.humintell.com/research-tools/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>Copyright</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Japanase and caucasion faces with 7 basic expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Japanese Female Facial Expression</th>
<td>Lyons, Michael, Kamachi, Miyuki, &amp; Gyoba, Jiro. (1998). The Japanese Female Facial Expression (JAFFE) Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3451524&nbsp;</td>
<td><a href="https://zenodo.org/record/3451524#.Y9lKHnbP0dV">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">10 Japanese females, 7 basic expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>Yes</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Karolinska Directed Emotional Faces</th>
<td>Lundqvist, D., Flykt, A., &amp; &Ouml;hman, A. (1998). The Karolinska Directed Emotional Faces &ndash; KDEF, CD ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet, ISBN 91-630-7164-9.&nbsp;</td>
<td><a href="https://www.emotionlab.se/kdef/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">4900 pictures of human facial expressions of emotion</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Libor Spacek's Facial Images Databases</th>
<td>D. Hond, L. Spacek `Distinctive Descriptions for Face Processing', Proceedings of the 8th British Machine Vision Conference BMVC97, Colchester, England, pp. 320-329, September 1997&nbsp;</td>
<td><a href="https://cmp.felk.cvut.cz/~spacelib/faces/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>Link to web page</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">395 individuals, 20 images per model, mostly ages 18-20</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Make up Datasets</th>
<td>A. Dantcheva, C. Chen, A. Ross, "Can Facial Cosmetics Affect the Matching Accuracy of&nbsp;Face Recognition Systems?," Proc. of 5th IEEE International Conference on Biometrics:&nbsp;Theory, Applications and Systems (BTAS), (Washington DC, USA), September 2012&nbsp;</td>
<td><a href="http://www.antitza.com/makeup-datasets.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
 <td colspan="2">Faces with and without make up</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Messiner African American and Caucasian Male Sets</th>
<td>Meissner, C. A., Brigham, J. C., &amp; Butz, D. A. (2005). Memory for own‐and other‐race faces: A dual‐process approach. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 19(5), 545-567.&nbsp;</td>
<td><a href="http://iilab.utep.edu/stimuli.htm">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Male, African American and Caucasian smiling/not smiling</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>Christian Meissener cmeissner@utep.edu</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Montreal Set of Facial Displays of Emotion</th>
<td>Varies&nbsp;</td>
<td><a href="http://www.psychophysiolab.com/en/msfde.php">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Expressions of happiness, sadness, anger, fear, disgust, and embarrassment as well as a neutral expression</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<td>Social Psychophysiology Laboratory,&nbsp;Universit&eacute; du Qu&eacute;bec &agrave; Montr&eacute;al</td>
</tr>
<tr>
<th rowspan="7">MMI Facial Expression Database</th>
<td>Valstar, M., &amp; Pantic, M. (2010, May). Induced disgust, happiness and surprise: an addition to the mmi facial expression database. In Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect (p. 65).&nbsp;</td>
<td><a href="https://mmifacedb.eu/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Videos and stills prototypical expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>Yes</td>
<td>Dr Yiming Lin</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">MR2 Face Database</th>
<td>Strohminger, N., Gray, K., Chituc, V., Heffner, J., Schein, C., and Heagins, T.B. (in press). The MR2: A multi-racial mega-resolution database of facial stimuli. Behavior Research Methods&nbsp;</td>
<td><a href="https://osf.io/skbq2/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">74 multi-racial, mega-resolution database of facial stimuli</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>Yes</td>
<td>NS</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">NimStim Set of Facial Expressions</th>
<td>Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., ... &amp; Nelson, C. (2009). The NimStim set of facial expressions: judgments from untrained research participants. Psychiatry Research, 168(3), 242-249.&nbsp;</td>
<td><a href="https://osf.io/y86rw/wiki/home/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">672 images of naturally posed photographs by 43 professional actors (18 female, 25 male)&nbsp;</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Psychological Image Collection at Sterling</th>
<td><a href="http://pics.psych.stir.ac.uk/ESRC/index.htm">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">45 male/54 female</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr>
<td>NS</td>
<td>Peter Hancock,&nbsp;pjbh1@stir.ac.uk</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Radboud Faces Database</th>
<td>Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., Hawk, S.T., &amp; van Knippenberg, A. (2010). Presentation and validation of the Radboud Faces Database.&nbsp;Cognition &amp; Emotion, 24(8),&nbsp;1377&mdash;1388. DOI: 10.1080/02699930903485076&nbsp;</td>
<td><a href="https://rafd.socsci.ru.nl/RaFD2/RaFD?p=main">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">67 models, caucasian, adults and children 8 emotional expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>

<tr>
<td>NS</td>
<td>info@rafd.nl</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">RADIATE</th>
<td>Conley, M. I., Dellarco, D. V., Rubien-Thomas, E., Cohen, A. O., Cervera, A., Tottenham, N., &amp; Casey, B. J. (2018). The racially diverse affective expression (RADIATE) face stimulus set.&nbsp;Psychiatry research.&nbsp;</td>
<td><a href="http://fablab.yale.edu/page/assays-tools">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">face stimulus set of 1721 racially diverse expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Sheffield Face Database</th>
<td>Wechsler, H., Phillips, J. P., Bruce, V., Soulie, F. F., &amp; Huang, T. S. (Eds.). (2012). Face recognition: From theory to applications (Vol. 163). Springer Science &amp; Business Media.&nbsp;</td>
<td><a href="https://www.visioneng.org.uk/datasets/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">564 images of 20 individuals (mixed race/gender/appearance)</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Todorov Synthetic Faces Databases</th>
<td>Varies&nbsp;</td>
<td><a href="https://tlab.uchicago.edu/databases/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Several databases of computer generated synthetic faces</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<td>https://www.chicagobooth.edu/faculty/directory/t/alexander-todorov</td>
</tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">UB KinFace</th>
<td>&nbsp;Siyu&nbsp;Xia, Ming&nbsp;Shao,&nbsp;Jiebo&nbsp;Luo, and&nbsp;Yun&nbsp;Fu, &ldquo;Understanding Kin Relationships in a Photo&rdquo;,&nbsp;IEEE Transactions on Multimedia&nbsp;(T-MM), Volume: 14, Issue: 4, Page(s): 1046-1056, 201&nbsp;</td>
<td><a href="http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">600 images of 400 people which can be separated into 200 groups. Each group is composed of child, young parent and old parent images</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Yale Face Database</th>
<td>Varies&nbsp;</td>
<td><a href="http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html#:~:text=The%20Yale%20Face%20Database%20(size,sleepy%2C%20surprised%2C%20and%20wink.">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Face</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">165 grayscale images in GIF format of 15 individuals</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">IUAV image dataset</th>
<td>Bertamini, M.&nbsp;&amp; Sinico, M. (2019).&nbsp;A study of objects with smooth or sharp features created as line drawings by&nbsp;individuals trained in design.&nbsp;Empirical Studies of the Arts,&nbsp;doi: 10.1177/0276237419897048</td>
<td><a href="https://osf.io/cx62j/">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Objects</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">772 images line drawings object classification, category (computer vs free hand drawing)</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Action Database</th>
<td>Keefe, B.D., Villing, M., Racey, C., Strong, S.M., Wincenciak, J., Barraclough, N.E. (2014) A database of whole-body action videos for the study of action, emotion and untrustworthiness trustworthiness discrimination.&nbsp;Behavior Research Methods&nbsp;doi:10.3758/s13428-013-0439-&nbsp;</td>
<td><a href="https://www-users.york.ac.uk/~neb506/databases.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Movement</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">whole-body action videos for the study of action, emotion, untrustworthiness, identity, and gender.</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Bodily Expressive Action Stimulus Test</th>
<td>de Gelder, B. &amp; Van den Stock, J. (2011). The Bodily Expressive Action Stimulus Test (BEAST). Construction and validation of a stimulus basis for measuring perception of whole body expression of emotions.&nbsp;Frontiers in Psychology&nbsp;2:181. doi:10.3389/fpsyg.2011.0018.&nbsp;</td>
<td><a href="http://www.beatricedegelder.com/beast.html">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>None</td>
<td>Movement</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Whole body expressions</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Speech Accent Archive</th>
<td>Weinberger, Steven. (2015). Speech Accent Archive. George Mason University. Retrieved from http://accent.gmu.edu&nbsp;</td>
<td><a href="http://accent.gmu.edu/howto.php">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC-SA 2.0</td>
<td>Audio</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">Database of accents</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Massive Auditory Lexical Decision Pseudowords</th>
<td>Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadić, F., &amp; Sims, M. (2019). The massive auditory lexical decision (MALD) database. Behavior research methods, 51, 1187-1204.&nbsp;</td>
<td><a href="https://doi.org/10.7939/r3-v7jh-p314 ">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Audio</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">data set for speech and psycholinguistic research, 9592 pseudowords</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>
<tr>
<th>Name</th>
<th>Reference</th>
<th>Link</th>
</tr>
<tr>
<th rowspan="7">Massive Auditory Lexical Decision Words</th>
<td>Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadić, F., &amp; Sims, M. (2019). The massive auditory lexical decision (MALD) database. Behavior research methods, 51, 1187-1204.&nbsp;</td>
<td><a href="https://doi.org/10.7939/r3-v0jr-rr12">LINK</a></td>
</tr>
<tr>
<th>Attribution</th>
<th>Type</th>
</tr>
<tr>
<td>CC BY-NC 4.0</td>
<td>Audio</td>
</tr>
<tr>
 <th colspan="2">Details</th>
</tr>
<tr>
<td colspan="2">data set for speech and psycholinguistic research, 26,793 words</td>
</tr>
<tr>
<th>Norms/Validity?</th>
<th>Contact</th>
</tr>
<tr><td>NS</td>
<td>NS</td></tr>

</tbody>
</table>
